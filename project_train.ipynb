{"cells":[{"cell_type":"markdown","source":["#Start"],"metadata":{"id":"byqeaZxJ8ayD"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QJrcP5yaN8NW","outputId":"2dd1c593-785c-40b6-9fdc-799852e29515","executionInfo":{"status":"ok","timestamp":1669257154633,"user_tz":-540,"elapsed":19927,"user":{"displayName":"hm k","userId":"10944937434822569932"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# 구글 드라이브 저장소 연결\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## transformers & Huggingface\n","\n","transformer 강의 (https://youtu.be/Yk1tV_cXMMU)\n","\n","huggingface <br>\n",": transformer model과 학습 스크립트를 제공하는 모듈 <br>\n","huggingface를 사용하면 bert, gpt 등 transformer 모델 사용시 layer, model 등을 선언하거나 구현하지 않아도 됨 "],"metadata":{"id":"jhl-KLZFFneI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oY0JDCeIOCwG"},"outputs":[],"source":["# transformer 설치\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUpCI2x3Pgqs"},"outputs":[],"source":["# import\n","import pandas as pd\n","import numpy as np\n","import random\n","import os\n","import math\n","import easydict\n","import warnings\n","warnings.filterwarnings('ignore') # waring 무시\n","\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import StratifiedKFold\n","\n","from tqdm import tqdm\n","tqdm.pandas()\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.optim.optimizer import Optimizer\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import AutoTokenizer, BertModel\n","from transformers import get_linear_schedule_with_warmup\n","\n","# 작업 디렉토리 변경\n","root_dir = \"/content/drive/MyDrive/\"\n","project_folder = \"project\"\n","os.chdir(os.path.join(root_dir,project_folder))"]},{"cell_type":"code","source":["# 재현성 확보를 위한 random seed 고정\n","def seed_everything(seed: int = 42, contain_cuda: bool = False):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    print(f\"Seed set as {seed}\")"],"metadata":{"id":"pLngIst2db9r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Arguments"],"metadata":{"id":"Jh-u1-2T8zEX"}},{"cell_type":"code","source":["# device 선택 gpu 사용 가능하다면 cuda, 아니면 cpu\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'current device : {device}')\n","\n","# arguments 설정\n","args = easydict.EasyDict({\n","        \"seed\":42,\n","        \"warmup_steps\":500,\n","        \"cycle_mult\":1.2,\n","        \"seq_max_len\":128,\n","        \"batch_size\": 32,\n","        \"epochs\": 10,\n","        \"patience\":3,\n","        \"n_splits\" : 5,\n","        \"lr\": 1e-05,\n","        \"num_workers\":2,\n","        \"smoothing\": 0.1,\n","        \"dp\": 0.1,\n","        \"train_file\":'train.csv',\n","    })\n","\n","project_name = \"project\"\n","args.update(\n","            {\n","                \"project_name\":project_name,\n","                \"model_name\":project_name,\n","             }\n","            )\n","\n","seed_everything(args.seed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crCtCjtsdWje","executionInfo":{"status":"ok","timestamp":1669257242689,"user_tz":-540,"elapsed":6,"user":{"displayName":"hm k","userId":"10944937434822569932"}},"outputId":"bcbd6138-15b5-4db3-cd9a-d32be7ea31e2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["current device : cuda\n","Seed set as 42\n"]}]},{"cell_type":"markdown","metadata":{"id":"GQf5iFF67Gdw"},"source":["#Data Processing\n",": 자연어 csv 형태의 data를 bert model의 input 형태로 변경하는 과정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MZRbHojjlmi"},"outputs":[],"source":["# Dataset 구성\n","class NewsDataset(Dataset):\n","    def __init__(self, tokenized_dataset, label):\n","        self.tokenized_dataset = tokenized_dataset\n","        self.label = label\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx].clone().detach() for key, val in self.tokenized_dataset.items()}\n","        item['label'] = torch.tensor(self.label[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.label)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7b0_5KGkGzc"},"outputs":[],"source":["# 기본적인 데이터 전처리\n","def preprocessing_dataset(args, dataset):\n","    dataset = dataset.loc[dataset.label.isnull()==False,:] # label 기준으로 결측치 있는 행 제거\n","    dataset = dataset.drop_duplicates(['title','content','label']) # 중복제거\n","    dataset = dataset.reset_index(drop=True)\n","    return dataset\n","\n","# 데이터 불러오기\n","def load_data(args, dataset_dir):\n","    dataset = pd.read_csv(dataset_dir) # load dataset\n","    dataset = preprocessing_dataset(args, dataset) # preprecessing dataset\n","    return dataset\n","\n","# bert input을 위한 tokenizing\n","def tokenized_dataset(args, dataset, tokenizer):\n","    lst_title = dataset['title'].tolist() # 제목 column to list\n","    lst_content = dataset['content'].tolist() # 내용 column to list\n","\n","    # 사전학습된 tokenizer를 활용해 data tokenizing\n","    tokenized_sentences = tokenizer(\n","        lst_title,\n","        lst_content,\n","        return_tensors=\"pt\", # token return type : Tensor\n","        padding=True, # max_len으로 padding\n","        truncation=True, # max_len 보다 길 경우 crop\n","        max_length=args.seq_max_len, # max_len\n","        add_special_tokens=True # <CLS>, <SEP> 등 special token 추가\n","    )\n","\n","    return tokenized_sentences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aY_uOAhyb_TC"},"outputs":[],"source":["# DataLoader? 전체 data batch size로 slice, mini batch 생성\n","def get_trainLoader(args, train_data, valid_data, train_label, valid_label, tokenizer):\n","    # bert input을 위한 tokenizing\n","    tokenized_train = tokenized_dataset(args, train_data, tokenizer)\n","    tokenized_valid = tokenized_dataset(args, valid_data, tokenizer)\n","\n","    # make dataset for pytorch.\n","    Newstrain_dataset = NewsDataset(tokenized_train, train_label)\n","    Newsvalid_dataset = NewsDataset(tokenized_valid, valid_label)\n","\n","    # DataLoader 사용해 train, valid mini batch 생성\n","    trainloader = DataLoader(Newstrain_dataset,\n","                             batch_size=args.batch_size,\n","                             shuffle=True,\n","                             num_workers=args.num_workers,\n","                             )\n","\n","    validloader = DataLoader(Newsvalid_dataset,\n","                             batch_size=args.batch_size,\n","                             shuffle=False,\n","                             num_workers=args.num_workers,\n","                             )\n","\n","    return trainloader, validloader"]},{"cell_type":"markdown","metadata":{"id":"LQnmL_4T6XZZ"},"source":["# Optimizer\n",": loss 최적화 알고리즘 선택"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kDco2rpcOOM"},"outputs":[],"source":["def get_optimizer(model, args):\n","    optimizer = AdamW(\n","        model.parameters(), \n","        lr=args.lr,\n","        weight_decay=0.01 # gradient descent에서 weight 업데이트를 할 때, 이전 weight의 크기를 일정 비율 감소시켜 과적합 방지\n","        )\n","    # 모든 parameter들의 grad값을 0으로 초기화\n","    optimizer.zero_grad()\n","    return optimizer"]},{"cell_type":"markdown","metadata":{"id":"_0jaQsk15Czi"},"source":["# Scheduler\n",": 학습 과정에서 lr 조절하는 scheduler 선택"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlfxJhaDdD52"},"outputs":[],"source":["def get_scheduler(optimizer, args, total_batch_):\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=args.warmup_steps,\n","        num_training_steps=int(total_batch_*args.epochs),\n","        )\n","    return scheduler"]},{"cell_type":"markdown","metadata":{"id":"SMlnGWx6OUlu"},"source":["# Model\n",": transformers의 BertModel 구조와 한국어 data로 사전학습 된 parameter를 불러온 후, <br> classification에 사용하기 위해 추가적인 layer을 쌓음.\n","\n","BERT 강의 (https://youtu.be/IwtexRHoWG0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N5PSNLnIdNw0"},"outputs":[],"source":["class kobert_Classifier(nn.Module):\n","    def __init__(self, bert, hidden_size=768, num_classes=2, dr_rate=0.0):\n","        super(kobert_Classifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","\n","        self.pooler = nn.Linear(hidden_size, hidden_size)\n","        self.classifier = nn.Linear(hidden_size, num_classes)\n","        torch.nn.init.xavier_uniform_(self.classifier.weight)\n","        self.dropout = nn.Dropout(p=dr_rate)\n","\n","    def forward(self, token_ids, attention_mask, segment_ids):\n","        out = self.bert(input_ids=token_ids, attention_mask=attention_mask, token_type_ids=segment_ids)[0]\n","        out = out[:, 0, :]\n","        out = self.pooler(out)\n","        out = torch.nn.functional.tanh(out)\n","\n","        if self.dr_rate:\n","            out = self.dropout(out)\n","        \n","        return self.classifier(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RKFQqbGdaPa"},"outputs":[],"source":["# 사전학습된 tokenizer import\n","def get_tokenizer(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\"kykim/bert-kor-base\")\n","    return tokenizer"]},{"cell_type":"code","source":["# 사전학습된 model, parameter import\n","def get_model(args) :\n","    feature_model = BertModel.from_pretrained(\"kykim/bert-kor-base\")\n","    model = kobert_Classifier(feature_model, dr_rate = args.dp)\n","    return model"],"metadata":{"id":"IZnAGE3zfzxJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ztb66gTmudIB"},"source":["# Loss\n",": 손실함수 선택"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JyNLsaOLdpew"},"outputs":[],"source":["# Label Smoothing은 Hard label(0, 1)을 Soft label(0과 1 사이의 값으로 구성)로 smoothing하는 것, 불균형 데이터 학습시 사용\n","# CrossEntrophy에 LabelSmoothing 적용한 loss\n","class LabelSmoothingLoss(nn.Module):\n","    def __init__(self, classes=2, smoothing=0.0, dim=-1):\n","        super(LabelSmoothingLoss, self).__init__()\n","        self.confidence = 1.0 - smoothing\n","        self.smoothing = smoothing\n","        self.cls = classes\n","        self.dim = dim\n","\n","    def forward(self, pred, target):\n","        pred = pred.log_softmax(dim=self.dim)\n","        with torch.no_grad():\n","            true_dist = torch.zeros_like(pred)\n","            true_dist.fill_(self.smoothing / (self.cls - 1))\n","            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n","\n","def get_criterion(args):\n","    criterion = LabelSmoothingLoss(smoothing=args.smoothing)\n","    return criterion"]},{"cell_type":"markdown","metadata":{"id":"xvXVqIs2Pmto"},"source":["# Train\n","- StratifiedKFold를 사용해 data 분할\n","- F1 Score 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HI8kIEEVds2r"},"outputs":[],"source":["def train(args, fold_lst=list(range(1, args.n_splits + 1))):\n","    criterion = get_criterion(args)\n","    tokenizer = get_tokenizer(args)\n","    all_dataset = load_data(args, dataset_dir = f'{args.train_file}')\n","    all_label = all_dataset['label'].values\n","\n","    kf = StratifiedKFold(n_splits=args.n_splits, random_state=42, shuffle=True)\n","    fold_idx = 1\n","    best_val_f1_list = []\n","    for train_index, test_index in kf.split(all_dataset, all_label):\n","        ###########################\n","        if fold_idx not in fold_lst:\n","            fold_idx+=1\n","            continue\n","        ###########################\n","\n","        os.makedirs(f'./models/{fold_idx}-fold', exist_ok=True)\n","        ### Model Select\n","        model = get_model(args)\n","        print('===================get model===================')\n","        model.to(device)\n","\n","        train_data, valid_data = all_dataset.iloc[train_index], all_dataset.iloc[test_index]\n","        train_label, valid_label = all_label[train_index], all_label[test_index]\n","        \n","        print(f\"len(train_label) : {len(train_label)}\")\n","        print(f\"len(valid_label) : {len(valid_label)}\")\n","\n","        trainloader, validloader = get_trainLoader(args, train_data, valid_data, train_label, valid_label, tokenizer)\t\n","        total_batch_, valid_batch_ = len(trainloader), len(validloader)\n","\n","        ### Optimizer\n","        optimizer = get_optimizer(model, args)\n","\n","        ### Scheduler\n","        scheduler = get_scheduler(optimizer, args, total_batch_)\n","\n","        best_val_loss, best_val_f1, = np.inf, 0\n","        early_stopping_counter = 0\n","\n","        print(f\"---------------------------------- {fold_idx} fold----------------------------------\")\t\n","        for i in tqdm(range(1, args.epochs+1)):\n","            model.train()\n","            epoch_perform, batch_perform = np.zeros(2), np.zeros(2)\t\n","            print()\n","            progress_bar = tqdm(enumerate(trainloader), total=len(trainloader), leave=True, position=0,)\n","            for j, v in progress_bar:\n","                input_ids, attention_mask, labels = v['input_ids'].to(device), v['attention_mask'].to(device), v['label'].to(device)\n","\n","                token_type_ids = None\n","\n","                optimizer.zero_grad()\n","\n","                output = model(input_ids, attention_mask, token_type_ids) ## label을 안 넣어서 logits값만 출력\t\n","\n","                loss = criterion(output, labels)\n","                loss.backward()\n","                optimizer.step()\n","                scheduler.step()\n","\n","                predict = output.argmax(dim=-1)\n","                predict = predict.detach().cpu().numpy()\n","                labels = labels.detach().cpu().numpy()\t\n","                f1 = f1_score(labels, predict)\n","\n","                batch_perform += np.array([loss.item(), f1])\n","                epoch_perform += np.array([loss.item(), f1])\n","\n","                if (j + 1) % 50 == 0:\n","                    print(\n","                        f\"Epoch {i:#04d} #{j + 1:#03d} -- loss: {batch_perform[0] / 50:#.5f}, f1: {batch_perform[1] / 50:#.4f}\"\n","                        )\n","                    batch_perform = np.zeros(2)\n","            print()\n","            print(\n","                f\"Epoch {i:#04d} loss: {epoch_perform[0] / total_batch_:#.5f}, f1: {epoch_perform[1] / total_batch_:#.2f}\"\n","                )\n","            \n","            ###### Validation\t\n","            model.eval()\n","            valid_perform = np.zeros(2)\n","\n","            all_valid_predict_lst = []\n","            all_valid_labels_lst = []\n","\n","            with torch.no_grad():\n","                for v in tqdm(validloader, total=valid_batch_, leave=True, position=0,):\n","                    input_ids, attention_mask, valid_labels = v['input_ids'].to(device), v['attention_mask'].to(device), v['label'].to(device)\n","\n","                    token_type_ids = None\n","\n","                    valid_output = model(input_ids, attention_mask, token_type_ids)\n","                    valid_loss = criterion(valid_output, valid_labels)\t\n","\n","                    valid_predict = valid_output.argmax(dim=-1)\n","                    valid_predict = valid_predict.detach().cpu().numpy()\n","                    valid_labels = valid_labels.detach().cpu().numpy()\t\n","\n","                    valid_f1 = f1_score(valid_labels, valid_predict)\t\n","                    valid_perform += np.array([valid_loss.item(), valid_f1])\n","\n","                    all_valid_predict_lst += list(valid_predict)\n","                    all_valid_labels_lst += list(valid_labels)\n","            \n","            ###### Model save\n","            val_total_loss = valid_perform[0] / valid_batch_\n","            val_total_f1 = valid_perform[1] / valid_batch_\n","            if best_val_loss > val_total_loss:\n","                best_val_loss = val_total_loss\n","                best_epoch_loss = i\n","        \n","            if val_total_f1 > best_val_f1 + 5e-04:    #  and val_total_f1 >= 0.33\n","                print(f\"New best model for val f1uracy : {val_total_f1:#.4f}! saving the best model..\")\n","                torch.save(model.state_dict(), f\"./models/{fold_idx}-fold/best.pt\")\n","                \n","                # 참고 : Model 추가 재학습을 위한 모델을 저장하는 코드\n","                # https://tutorials.pytorch.kr/beginner/saving_loading_models.html#checkpoint\n","\n","                best_val_f1 = val_total_f1\n","                best_epoch_f1 = i\n","                early_stopping_counter = 0\n","            \n","            else: # best보다 score가 안 좋을 때, early stopping check\n","                early_stopping_counter += 1\n","                if early_stopping_counter >= args.patience:\n","                    print(\n","                        f\"EarlyStopping counter: {early_stopping_counter} out of {args.patience}\"\n","                    )\n","                    break\n","\n","            print()\n","            print(\n","                f\">>>> Validation loss: {val_total_loss:#.5f}, f1: {val_total_f1:#.4f}\"\n","                )\n","            print()\n","\n","        best_val_f1_list.append(best_val_f1)\n","        fold_idx +=1\n","    print('='*50)\n","    print(f\"{args.n_splits}-fold best_val_f1_list : {best_val_f1_list}\")\n","    print('='*15, f'{args.n_splits}-fold Final Score(f1) : {np.mean(best_val_f1_list)}', '='*15)"]},{"cell_type":"markdown","metadata":{"id":"AkzISGF8gdKY"},"source":["# Training"]},{"cell_type":"code","source":["args"],"metadata":{"id":"OwCbLeLUH_tW"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfIRPVeVTdOY"},"outputs":[],"source":["train(args, fold_lst=list(range(1, args.n_splits + 1)))"]},{"cell_type":"code","source":["model = get_model(args)\n","torch.save(model, './models/model.pt')"],"metadata":{"id":"YS_U3h_ZxgAd"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1ya9C_nkqSzwRcmodLiY7ZwwW2Mu6dsMI","timestamp":1669255571954}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}